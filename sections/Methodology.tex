\documentclass[../main/main.tex]{subfiles}

\begin{document}
	
	\section{ Methodology}
	To address the problem mentioned in section \ref{statement}, This research will be using the system architecture proposed in \shortcite{Hu2018} with some modifications. This architecture is shown in Figure \ref{system}. This system is chosen because it is more reliable architecture when it comes to recovery of message  compared to \shortcite{Zhang2019}. This error of recovery can be further improved by adding some form of error recovery code like Reed-Solomon. If time permits this issue will be addressed. However it is optional. 
	
	In addition to this, other consideration is taken with regard to how much of an improvement could the change of \gls{GAN} would have on the final result. This eliminates the system proposed in \shortcite{Ke}. The choice of \gls{infoGAN} in \shortcite{Ke} is crucial for overall working of the system. 
	
	\subfile{../images/proposedArchitecture}
	
	As depicted in Figure \ref{phase:one} before the communication between Alice and Bob starts Alice will train a generator. This generator is a \gls{GAN}, which will try to generate the most realistic images it can come up with.
	After the generator is trained to its convergence; Alice will train the extractor by using the generator's output, and the error between recovered noise vector $z^\prime$ from the output of extractor and the initial noise vector $z$. This second phase is shown in Figure \ref{phase:two}. When this is done Alice will have both a generator and an extractor that act as  encoder and  decoder respectively. The extractor will be sent to Bob and steganographic communication can start between Alice and Bob.
	
	According to \shortcite{Cachin1998},  if the relative entropy between  probability distribution of  cover image and probability distribution of stego image is less than $\varepsilon$ the system is considered to be $\varepsilon$-secure. \shortcite{Zhang2019} extended this method to generative steganography as follows. 
		
	\theoremstyle{definition}
	\begin{definition}{$\varepsilon$-secure generative steganography: }
		If the generated images have sufficiently similar probability distribution as that of the real dataset,  the system is considered to be secure. This relation is shown in equation \ref{esecure}.
		\begin{equation} \label{esecure}
		\mathcal{D}(P_{generated},P_{real}) \leq \varepsilon
		\end{equation}
		
		If $\varepsilon=0$ then the system is said to be perfectly secure. 
	\end{definition}

	\shortcite{Zhang2019} used the Jensen-Shannon divergence ($D_{JS}$) as the function $\mathcal{D}(\cdot)$. However, \gls{DCGAN} will not be used in this research. So the usage of $D_{JS}$ will not be feasible. Therefore other metrics to calculate the divergence will be investigated, and would be selected based on the choice of \gls{GAN}.  
	
	The next important question that must be answered in this research is the choice of \gls{GAN}. For this the following parameters have been taken into consideration:
	\begin{enumerate}
		\item Stability,
		\item Convergence speed,
		\item Quality of  generated image 
		\item And most importantly how the divergence between the probability distributions is measured in the proposed \gls{GAN}.
	\end{enumerate}

	The overall methedology of this research is to implement the proposed architecture with different \gls{GAN}s and test for the above mentioned parameters. Even though more research is going to be conducted in the coming months, the following \gls{GAN}s must be tested since they have good performance in terms of the above mentioned parameters. 
	
	\begin{itemize}
		\item \gls{BEGAN} \shortcite{berthelot2017began} 
		\item \gls{RGAN} \shortcite{jolicoeur2018relativistic}
		\item  \gls{WGAN-DIV} \shortcite{wu2018wasserstein}
		\item \gls{WGAN-GP} \shortcite{gulrajani2017improved}
	\end{itemize}

	In any machine learning research selecting the dataset for training is crucial. In this research the following points will be considered as a guidance in the selection process. 
	
	\begin{enumerate}
		\item Availability of the dataset.
		\item Usage of the dataset in previous researches.
		\item Feature richness and diversity
	\end{enumerate}
	Based on this celebA \shortcite{liu2015faceattributes} dataset has no competitor.  It has 202,599 face images, with more than 40 binary attributes. In addition it has been used in \shortcite{Zhang2019}, \shortcite{Hu2018} and \shortcite{Ke}. This dataset is available at \url{http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html}. Some other datasets that are previously used in similar research include MNIST \shortcite{mnist}, Food101 \shortcite{bossard14}, and LFW \shortcite{LFWTech}. All of them are available freely. 
	
	Another important method that needs to be prepared is a testing method for comparison. Testing for convergence speed and stability is straight forward. It can be interpreted from the error loss vs training epoch graph. However, measuring for image quality is somewhat a difficult challenge. In most papers \shortcite{Jolicoeur-Martineau2019}, \shortcite{arjovsky2017wasserstein}, \shortcite{gulrajani2017improved} visual inspection of image fidelity is used. \shortcite{Li2018BEGANs} used inception score to measure the quality and diversity of the generated images. There are some other testing methods for image quality that might assist in this research. This include visual turing test \shortcite{geman2015visual} and \gls{MOS} (mean opinion score) \shortcite{streijl2016mean}. More research will be conducted  into this methods and a meaningful method will be selected for comparison.
\end{document}